
# Paper list of Transformer accelerators

(2020) FTRANS: Energy-Efficient Acceleration of Transformers using FPGA ([link](https://arxiv.org/pdf/2007.08563.pdf))

(2020) A3: Accelerating Attention Mechanisms in Neural Networks with Approximation ([link](https://arxiv.org/pdf/2002.10941.pdf))

(2020) HybridDNN: A Framework for High-Performance Hybrid DNN Accelerator Design and Implementation ([link](https://arxiv.org/pdf/2004.03804.pdf))

(2021) SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning ([link](https://arxiv.org/pdf/2012.09852.pdf))

(2021) ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks ([link](https://taejunham.github.io/data/elsa_isca21.pdf))

(2022) Energon: Towards Efficient Acceleration of Transformers Using Dynamic Sparse Attention ([link](https://arxiv.org/pdf/2110.09310.pdf))

(2022) A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA Through Sparse Attention and Dynamic Pipelining ([link](https://arxiv.org/pdf/2208.03646.pdf))

(2023) Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference ([link](https://arxiv.org/pdf/2312.15159.pdf))

(2023) FLAT: An Optimized Dataflow forMitigating Attention Bottlenecks ([link](https://people.csail.mit.edu/suvinay/pubs/2023.flat.asplos.pdf))
